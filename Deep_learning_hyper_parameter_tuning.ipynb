{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satishgc227/Derby-project/blob/main/Deep_learning_hyper_parameter_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTFysGRca-x5",
        "outputId": "d304860c-5063-463a-c206-65263982cdfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: bayesian-optimization in /usr/local/lib/python3.7/dist-packages (1.3.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.2.0)\n"
          ]
        }
      ],
      "source": [
        "# Import packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, BatchNormalization, Dropout\n",
        "from keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from math import floor\n",
        "from sklearn.metrics import make_scorer, accuracy_score\n",
        "\n",
        "\n",
        "!pip install bayesian-optimization\n",
        "from bayes_opt import BayesianOptimization\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from keras.layers import LeakyReLU\n",
        "LeakyReLU = LeakyReLU(alpha=0.1)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option(\"display.max_columns\", None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Wck7ge22bc8_"
      },
      "outputs": [],
      "source": [
        "# Make scorer accuracy\n",
        "score_acc = make_scorer(accuracy_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guQ9yYh4bdoA",
        "outputId": "353ed7cc-43ad-4456-e319-dc22dce0710a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array(['ALW', 'AOC', 'CLM', 'SST'], dtype=object), array(['D', 'I', 'M', 'O', 'T'], dtype=object), array(['FM ', 'FT ', 'GD ', 'MY ', 'SY '], dtype=object), array(['AQU', 'BEL', 'SAR'], dtype=object)]\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: movecolumn in /usr/local/lib/python3.7/dist-packages (0.0.7)\n"
          ]
        }
      ],
      "source": [
        "# Load in the data\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "df = pd.read_csv('/content/edited complete.csv',encoding='latin',nrows=10000)\n",
        "\n",
        "df['race_type\\xa0'].unique()\n",
        "df['course_type'].unique()\n",
        "df['track_condition\\xa0'].unique()\n",
        "df['track_id'].unique()\n",
        "\n",
        "s=df[['race_type\\xa0','course_type','track_condition\\xa0','track_id']]\n",
        "\n",
        "ohe=OneHotEncoder()\n",
        "feature_array=ohe.fit_transform(df[['race_type\\xa0','course_type','track_condition\\xa0','track_id']]).toarray()\n",
        "\n",
        "feature_labels=ohe.categories_\n",
        "print(feature_labels)\n",
        "\n",
        "np.hstack(feature_labels)\n",
        "\n",
        "feature_labels=np.hstack(feature_labels)\n",
        "f=pd.DataFrame(feature_array,columns=feature_labels)\n",
        "pd.concat([df,f],axis=1)\n",
        "\n",
        "df.drop(['race_date','track_id','course_type','track_condition\\xa0','race_type\\xa0','jockey\\xa0','program_number\\xa0','latitude\\xa0','longitude\\xa0'],axis=1,inplace=True)\n",
        "df=pd.concat([df,f],axis=1)\n",
        "\n",
        "df.isnull().any()\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"Removing the null values\"\"\"\n",
        "\n",
        "df.replace([np.inf,-np.inf],np.nan ,inplace=True)\n",
        "df.dropna(inplace=True)\n",
        "df.isnull().any()\n",
        "\n",
        "!pip install movecolumn\n",
        "import movecolumn as mc\n",
        "mc.MoveToLast(df,'position_at_finish\\xa0')\n",
        "\n",
        "X = df.iloc[:,:-1]\n",
        "y = df.iloc[:, -1]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "feu4KRNycXeN"
      },
      "outputs": [],
      "source": [
        "from sklearn import model_selection\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size= 0.2, random_state = 1)\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "\n",
        "# Create function\n",
        "def nn_cl_bo(neurons, activation, optimizer, learning_rate,  batch_size, epochs ):\n",
        "    optimizerL = ['SGD', 'Adam', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl','SGD']\n",
        "    optimizerD= {'Adam':Adam(lr=learning_rate), 'SGD':SGD(lr=learning_rate),\n",
        "                 'RMSprop':RMSprop(lr=learning_rate), 'Adadelta':Adadelta(lr=learning_rate),\n",
        "                 'Adagrad':Adagrad(lr=learning_rate), 'Adamax':Adamax(lr=learning_rate),\n",
        "                 'Nadam':Nadam(lr=learning_rate), 'Ftrl':Ftrl(lr=learning_rate)}\n",
        "    activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
        "                   'elu', 'exponential', LeakyReLU,'relu']\n",
        "    neurons = round(neurons)\n",
        "    activation = activationL[round(activation)]\n",
        "    batch_size = round(batch_size)\n",
        "    epochs = round(epochs)\n",
        "    def nn_cl_fun():\n",
        "        opt = Adam(lr = learning_rate)\n",
        "        nn = Sequential()\n",
        "        nn.add(Dense(neurons, input_dim=10, activation=activation))\n",
        "        nn.add(Dense(neurons, activation=activation))\n",
        "        nn.add(Dense(1, activation='sigmoid'))\n",
        "        nn.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "        return nn\n",
        "    es = EarlyStopping(monitor='accuracy', mode='max', verbose=0, patience=20)\n",
        "    nn = KerasClassifier(build_fn=nn_cl_fun, epochs=epochs, batch_size=batch_size,\n",
        "                         verbose=0)\n",
        "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
        "    score = cross_val_score(nn, X_train, y_train, scoring=score_acc, cv=kfold, fit_params={'callbacks':[es]}).mean()\n",
        "    return score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdA3QOXEl019",
        "outputId": "997d2cd3-1e4e-403d-dbbb-d0b6361028ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "race_number            False\n",
              "trakus_index           False\n",
              "distance_id            False\n",
              "run_up_distance        False\n",
              "purse                  False\n",
              "post_time              False\n",
              "weight_carried         False\n",
              "odds                   False\n",
              "ALW                    False\n",
              "AOC                    False\n",
              "CLM                    False\n",
              "SST                    False\n",
              "D                      False\n",
              "I                      False\n",
              "M                      False\n",
              "O                      False\n",
              "T                      False\n",
              "FM                     False\n",
              "FT                     False\n",
              "GD                     False\n",
              "MY                     False\n",
              "SY                     False\n",
              "AQU                    False\n",
              "BEL                    False\n",
              "SAR                    False\n",
              "position_at_finish     False\n",
              "dtype: bool"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "df.replace([np.inf,-np.inf],np.nan ,inplace=True)\n",
        "df.dropna(inplace=True)\n",
        "df.isnull().any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eK409LBrdJH6"
      },
      "outputs": [],
      "source": [
        "# Set paramaters\n",
        "#params_nn ={\n",
        " #   'neurons': (10, 100),\n",
        " #   'activation':(0, 9),\n",
        "  #  'optimizer':(0,7),\n",
        "   # 'learning_rate':(0.01, 1),\n",
        "    #'batch_size':(200, 1000),\n",
        "   # 'epochs':(20, 100)\n",
        "#}\n",
        "# Run Bayesian Optimization\n",
        "#nn_bo = BayesianOptimization(nn_cl_bo, params_nn, random_state=111)\n",
        "#nn_bo.maximize(init_points=25, n_iter=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ydWRri2vdKg2"
      },
      "outputs": [],
      "source": [
        "#Here are the best hyperparameters.\n",
        "\n",
        "#params_nn_ = nn_bo.max['params']\n",
        "#activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
        " #              'elu', 'exponential', LeakyReLU,'relu']\n",
        "#params_nn_['activation'] = activationL[round(params_nn_['activation'])]\n",
        "#params_nn_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "X0tdg1_qdTcE"
      },
      "outputs": [],
      "source": [
        "#The following code creates a function for tuning the Neural Network hyperparameters and layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 974
        },
        "id": "wq9zrapbdaKe",
        "outputId": "de52335e-af04-4831-b058-c3db34344192"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   | activa... | batch_... |  dropout  | dropou... |  epochs   |  layers1  |  layers2  | learni... |  neurons  | normal... | optimizer |\n",
            "-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "| \u001b[0m1        \u001b[0m | \u001b[0mnan      \u001b[0m | \u001b[0m5.51     \u001b[0m | \u001b[0m335.3    \u001b[0m | \u001b[0m0.4361   \u001b[0m | \u001b[0m0.2308   \u001b[0m | \u001b[0m43.63    \u001b[0m | \u001b[0m1.298    \u001b[0m | \u001b[0m1.045    \u001b[0m | \u001b[0m0.426    \u001b[0m | \u001b[0m31.48    \u001b[0m | \u001b[0m0.3377   \u001b[0m | \u001b[0m6.935    \u001b[0m |\n",
            "| \u001b[0m2        \u001b[0m | \u001b[0mnan      \u001b[0m | \u001b[0m2.14     \u001b[0m | \u001b[0m265.0    \u001b[0m | \u001b[0m0.6696   \u001b[0m | \u001b[0m0.1864   \u001b[0m | \u001b[0m41.94    \u001b[0m | \u001b[0m1.932    \u001b[0m | \u001b[0m1.237    \u001b[0m | \u001b[0m0.08322  \u001b[0m | \u001b[0m91.07    \u001b[0m | \u001b[0m0.794    \u001b[0m | \u001b[0m5.884    \u001b[0m |\n",
            "| \u001b[0m3        \u001b[0m | \u001b[0mnan      \u001b[0m | \u001b[0m7.337    \u001b[0m | \u001b[0m992.8    \u001b[0m | \u001b[0m0.5773   \u001b[0m | \u001b[0m0.2441   \u001b[0m | \u001b[0m53.71    \u001b[0m | \u001b[0m1.055    \u001b[0m | \u001b[0m1.908    \u001b[0m | \u001b[0m0.1143   \u001b[0m | \u001b[0m83.55    \u001b[0m | \u001b[0m0.6977   \u001b[0m | \u001b[0m3.957    \u001b[0m |\n",
            "| \u001b[0m4        \u001b[0m | \u001b[0mnan      \u001b[0m | \u001b[0m2.468    \u001b[0m | \u001b[0m998.8    \u001b[0m | \u001b[0m0.138    \u001b[0m | \u001b[0m0.1846   \u001b[0m | \u001b[0m58.8     \u001b[0m | \u001b[0m1.81     \u001b[0m | \u001b[0m2.456    \u001b[0m | \u001b[0m0.3296   \u001b[0m | \u001b[0m46.05    \u001b[0m | \u001b[0m0.319    \u001b[0m | \u001b[0m6.631    \u001b[0m |\n",
            "| \u001b[0m5        \u001b[0m | \u001b[0mnan      \u001b[0m | \u001b[0m8.268    \u001b[0m | \u001b[0m851.1    \u001b[0m | \u001b[0m0.03408  \u001b[0m | \u001b[0m0.283    \u001b[0m | \u001b[0m96.04    \u001b[0m | \u001b[0m2.613    \u001b[0m | \u001b[0m1.963    \u001b[0m | \u001b[0m0.9671   \u001b[0m | \u001b[0m47.53    \u001b[0m | \u001b[0m0.3188   \u001b[0m | \u001b[0m0.1151   \u001b[0m |\n",
            "| \u001b[0m6        \u001b[0m | \u001b[0mnan      \u001b[0m | \u001b[0m0.3436   \u001b[0m | \u001b[0m242.5    \u001b[0m | \u001b[0m0.128    \u001b[0m | \u001b[0m0.01001  \u001b[0m | \u001b[0m38.11    \u001b[0m | \u001b[0m2.088    \u001b[0m | \u001b[0m1.357    \u001b[0m | \u001b[0m0.1876   \u001b[0m | \u001b[0m23.47    \u001b[0m | \u001b[0m0.683    \u001b[0m | \u001b[0m3.283    \u001b[0m |\n",
            "| \u001b[0m7        \u001b[0m | \u001b[0mnan      \u001b[0m | \u001b[0m6.914    \u001b[0m | \u001b[0m735.1    \u001b[0m | \u001b[0m0.4413   \u001b[0m | \u001b[0m0.1786   \u001b[0m | \u001b[0m56.93    \u001b[0m | \u001b[0m2.927    \u001b[0m | \u001b[0m1.296    \u001b[0m | \u001b[0m0.9077   \u001b[0m | \u001b[0m54.81    \u001b[0m | \u001b[0m0.5925   \u001b[0m | \u001b[0m4.793    \u001b[0m |\n",
            "| \u001b[0m8        \u001b[0m | \u001b[0mnan      \u001b[0m | \u001b[0m1.597    \u001b[0m | \u001b[0m891.7    \u001b[0m | \u001b[0m0.4821   \u001b[0m | \u001b[0m0.0208   \u001b[0m | \u001b[0m49.18    \u001b[0m | \u001b[0m1.723    \u001b[0m | \u001b[0m1.944    \u001b[0m | \u001b[0m0.1877   \u001b[0m | \u001b[0m25.78    \u001b[0m | \u001b[0m0.9491   \u001b[0m | \u001b[0m4.59     \u001b[0m |\n",
            "| \u001b[0m9        \u001b[0m | \u001b[0mnan      \u001b[0m | \u001b[0m1.215    \u001b[0m | \u001b[0m942.2    \u001b[0m | \u001b[0m0.8418   \u001b[0m | \u001b[0m0.01583  \u001b[0m | \u001b[0m36.29    \u001b[0m | \u001b[0m2.745    \u001b[0m | \u001b[0m2.348    \u001b[0m | \u001b[0m0.3043   \u001b[0m | \u001b[0m76.1     \u001b[0m | \u001b[0m0.6183   \u001b[0m | \u001b[0m1.473    \u001b[0m |\n",
            "| \u001b[0m10       \u001b[0m | \u001b[0mnan      \u001b[0m | \u001b[0m7.219    \u001b[0m | \u001b[0m247.3    \u001b[0m | \u001b[0m0.3082   \u001b[0m | \u001b[0m0.06221  \u001b[0m | \u001b[0m97.78    \u001b[0m | \u001b[0m2.819    \u001b[0m | \u001b[0m2.353    \u001b[0m | \u001b[0m0.124    \u001b[0m | \u001b[0m96.22    \u001b[0m | \u001b[0m0.09171  \u001b[0m | \u001b[0m4.409    \u001b[0m |\n",
            "| \u001b[0m11       \u001b[0m | \u001b[0mnan      \u001b[0m | \u001b[0m8.126    \u001b[0m | \u001b[0m471.8    \u001b[0m | \u001b[0m0.6528   \u001b[0m | \u001b[0m0.2775   \u001b[0m | \u001b[0m49.92    \u001b[0m | \u001b[0m2.543    \u001b[0m | \u001b[0m2.792    \u001b[0m | \u001b[0m0.624    \u001b[0m | \u001b[0m23.6     \u001b[0m | \u001b[0m0.3749   \u001b[0m | \u001b[0m4.451    \u001b[0m |\n",
            "| \u001b[0m12       \u001b[0m | \u001b[0mnan      \u001b[0m | \u001b[0m4.132    \u001b[0m | \u001b[0m625.8    \u001b[0m | \u001b[0m0.3523   \u001b[0m | \u001b[0m0.198    \u001b[0m | \u001b[0m58.12    \u001b[0m | \u001b[0m1.909    \u001b[0m | \u001b[0m1.25     \u001b[0m | \u001b[0m0.4183   \u001b[0m | \u001b[0m34.58    \u001b[0m | \u001b[0m0.3467   \u001b[0m | \u001b[0m6.821    \u001b[0m |\n",
            "| \u001b[0m13       \u001b[0m | \u001b[0mnan      \u001b[0m | \u001b[0m1.94     \u001b[0m | \u001b[0m746.3    \u001b[0m | \u001b[0m0.03181  \u001b[0m | \u001b[0m0.2506   \u001b[0m | \u001b[0m76.13    \u001b[0m | \u001b[0m2.932    \u001b[0m | \u001b[0m2.184    \u001b[0m | \u001b[0m0.2252   \u001b[0m | \u001b[0m74.73    \u001b[0m | \u001b[0m0.03087  \u001b[0m | \u001b[0m2.931    \u001b[0m |\n",
            "| \u001b[0m14       \u001b[0m | \u001b[0mnan      \u001b[0m | \u001b[0m2.531    \u001b[0m | \u001b[0m285.0    \u001b[0m | \u001b[0m0.4263   \u001b[0m | \u001b[0m0.2522   \u001b[0m | \u001b[0m28.83    \u001b[0m | \u001b[0m2.973    \u001b[0m | \u001b[0m1.467    \u001b[0m | \u001b[0m0.7242   \u001b[0m | \u001b[0m69.48    \u001b[0m | \u001b[0m0.07776  \u001b[0m | \u001b[0m4.881    \u001b[0m |\n",
            "| \u001b[0m15       \u001b[0m | \u001b[0mnan      \u001b[0m | \u001b[0m2.388    \u001b[0m | \u001b[0m921.5    \u001b[0m | \u001b[0m0.8183   \u001b[0m | \u001b[0m0.1198   \u001b[0m | \u001b[0m85.62    \u001b[0m | \u001b[0m1.396    \u001b[0m | \u001b[0m2.045    \u001b[0m | \u001b[0m0.4184   \u001b[0m | \u001b[0m93.33    \u001b[0m | \u001b[0m0.8254   \u001b[0m | \u001b[0m3.507    \u001b[0m |\n",
            "| \u001b[0m16       \u001b[0m | \u001b[0mnan      \u001b[0m | \u001b[0m1.051    \u001b[0m | \u001b[0m209.3    \u001b[0m | \u001b[0m0.9132   \u001b[0m | \u001b[0m0.1537   \u001b[0m | \u001b[0m87.45    \u001b[0m | \u001b[0m1.19     \u001b[0m | \u001b[0m2.607    \u001b[0m | \u001b[0m0.07161  \u001b[0m | \u001b[0m67.19    \u001b[0m | \u001b[0m0.9688   \u001b[0m | \u001b[0m2.782    \u001b[0m |\n",
            "| \u001b[0m17       \u001b[0m | \u001b[0mnan      \u001b[0m | \u001b[0m5.936    \u001b[0m | \u001b[0m371.9    \u001b[0m | \u001b[0m0.8899   \u001b[0m | \u001b[0m0.296    \u001b[0m | \u001b[0m79.09    \u001b[0m | \u001b[0m2.283    \u001b[0m | \u001b[0m1.504    \u001b[0m | \u001b[0m0.4811   \u001b[0m | \u001b[0m34.13    \u001b[0m | \u001b[0m0.8683   \u001b[0m | \u001b[0m1.868    \u001b[0m |\n",
            "| \u001b[0m18       \u001b[0m | \u001b[0mnan      \u001b[0m | \u001b[0m8.757    \u001b[0m | \u001b[0m370.8    \u001b[0m | \u001b[0m0.2978   \u001b[0m | \u001b[0m0.221    \u001b[0m | \u001b[0m21.03    \u001b[0m | \u001b[0m1.06     \u001b[0m | \u001b[0m2.468    \u001b[0m | \u001b[0m0.5033   \u001b[0m | \u001b[0m29.63    \u001b[0m | \u001b[0m0.00893  \u001b[0m | \u001b[0m5.955    \u001b[0m |\n",
            "| \u001b[0m19       \u001b[0m | \u001b[0mnan      \u001b[0m | \u001b[0m4.828    \u001b[0m | \u001b[0m778.8    \u001b[0m | \u001b[0m0.6616   \u001b[0m | \u001b[0m0.2516   \u001b[0m | \u001b[0m51.06    \u001b[0m | \u001b[0m1.852    \u001b[0m | \u001b[0m2.656    \u001b[0m | \u001b[0m0.4743   \u001b[0m | \u001b[0m83.8     \u001b[0m | \u001b[0m0.01418  \u001b[0m | \u001b[0m2.777    \u001b[0m |\n",
            "| \u001b[0m20       \u001b[0m | \u001b[0mnan      \u001b[0m | \u001b[0m1.155    \u001b[0m | \u001b[0m294.5    \u001b[0m | \u001b[0m0.206    \u001b[0m | \u001b[0m0.2243   \u001b[0m | \u001b[0m94.41    \u001b[0m | \u001b[0m1.761    \u001b[0m | \u001b[0m1.921    \u001b[0m | \u001b[0m0.8746   \u001b[0m | \u001b[0m83.31    \u001b[0m | \u001b[0m0.02497  \u001b[0m | \u001b[0m6.111    \u001b[0m |\n",
            "| \u001b[0m21       \u001b[0m | \u001b[0mnan      \u001b[0m | \u001b[0m5.441    \u001b[0m | \u001b[0m613.2    \u001b[0m | \u001b[0m0.5893   \u001b[0m | \u001b[0m0.2399   \u001b[0m | \u001b[0m33.86    \u001b[0m | \u001b[0m1.374    \u001b[0m | \u001b[0m1.516    \u001b[0m | \u001b[0m0.06056  \u001b[0m | \u001b[0m59.74    \u001b[0m | \u001b[0m0.3518   \u001b[0m | \u001b[0m6.419    \u001b[0m |\n",
            "| \u001b[0m22       \u001b[0m | \u001b[0mnan      \u001b[0m | \u001b[0m4.289    \u001b[0m | \u001b[0m283.6    \u001b[0m | \u001b[0m0.1525   \u001b[0m | \u001b[0m0.08206  \u001b[0m | \u001b[0m82.52    \u001b[0m | \u001b[0m1.786    \u001b[0m | \u001b[0m2.598    \u001b[0m | \u001b[0m0.4387   \u001b[0m | \u001b[0m17.34    \u001b[0m | \u001b[0m0.01064  \u001b[0m | \u001b[0m3.016    \u001b[0m |\n",
            "| \u001b[0m23       \u001b[0m | \u001b[0mnan      \u001b[0m | \u001b[0m5.966    \u001b[0m | \u001b[0m612.2    \u001b[0m | \u001b[0m0.5801   \u001b[0m | \u001b[0m0.1479   \u001b[0m | \u001b[0m79.24    \u001b[0m | \u001b[0m2.579    \u001b[0m | \u001b[0m2.562    \u001b[0m | \u001b[0m0.1363   \u001b[0m | \u001b[0m94.61    \u001b[0m | \u001b[0m0.8777   \u001b[0m | \u001b[0m4.897    \u001b[0m |\n",
            "| \u001b[0m24       \u001b[0m | \u001b[0mnan      \u001b[0m | \u001b[0m8.432    \u001b[0m | \u001b[0m739.0    \u001b[0m | \u001b[0m0.5944   \u001b[0m | \u001b[0m0.1035   \u001b[0m | \u001b[0m26.69    \u001b[0m | \u001b[0m2.159    \u001b[0m | \u001b[0m1.035    \u001b[0m | \u001b[0m0.5569   \u001b[0m | \u001b[0m66.93    \u001b[0m | \u001b[0m0.6784   \u001b[0m | \u001b[0m1.194    \u001b[0m |\n",
            "| \u001b[0m25       \u001b[0m | \u001b[0mnan      \u001b[0m | \u001b[0m5.194    \u001b[0m | \u001b[0m364.8    \u001b[0m | \u001b[0m0.2515   \u001b[0m | \u001b[0m0.2908   \u001b[0m | \u001b[0m91.73    \u001b[0m | \u001b[0m1.246    \u001b[0m | \u001b[0m2.762    \u001b[0m | \u001b[0m0.9485   \u001b[0m | \u001b[0m51.39    \u001b[0m | \u001b[0m0.413    \u001b[0m | \u001b[0m4.04     \u001b[0m |\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                 \u001b[0mx_probe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Queue is empty, no more objects to retrieve.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mStopIteration\u001b[0m: Queue is empty, no more objects to retrieve.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-fc88b9537bed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# Run Bayesian Optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mnn_bo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBayesianOptimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_cl_bo2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_nn2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m111\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mnn_bo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m                 \u001b[0mx_probe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_probe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36msuggest\u001b[0;34m(self, utility_function)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_constrained\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                 self.constraint.fit(self._space.params,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/gaussian_process/_gpr.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0my_numeric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_2d\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m         )\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    977\u001b[0m     )\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmulti_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_numeric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_numeric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_check_y\u001b[0;34m(y, multi_output, y_numeric)\u001b[0m\n\u001b[1;32m    988\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         y = check_array(\n\u001b[0;32m--> 990\u001b[0;31m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m         )\n\u001b[1;32m    992\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"allow-nan\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    114\u001b[0m             raise ValueError(\n\u001b[1;32m    115\u001b[0m                 msg_err.format(\n\u001b[0;32m--> 116\u001b[0;31m                     \u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m                 )\n\u001b[1;32m    118\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
          ]
        }
      ],
      "source": [
        "# Create function\n",
        "def nn_cl_bo2(neurons, activation, optimizer, learning_rate, batch_size, epochs,\n",
        "              layers1, layers2, normalization, dropout, dropout_rate):\n",
        "    optimizerL = ['SGD', 'Adam', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl','SGD']\n",
        "    optimizerD= {'Adam':Adam(lr=learning_rate), 'SGD':SGD(lr=learning_rate),\n",
        "                 'RMSprop':RMSprop(lr=learning_rate), 'Adadelta':Adadelta(lr=learning_rate),\n",
        "                 'Adagrad':Adagrad(lr=learning_rate), 'Adamax':Adamax(lr=learning_rate),\n",
        "                 'Nadam':Nadam(lr=learning_rate), 'Ftrl':Ftrl(lr=learning_rate)}\n",
        "    activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
        "                   'elu', 'exponential', LeakyReLU,'relu']\n",
        "    neurons = round(neurons)\n",
        "    activation = activationL[round(activation)]\n",
        "    optimizer = optimizerD[optimizerL[round(optimizer)]]\n",
        "    batch_size = round(batch_size)\n",
        "    epochs = round(epochs)\n",
        "    layers1 = round(layers1)\n",
        "    layers2 = round(layers2)\n",
        "    def nn_cl_fun():\n",
        "        nn = Sequential()\n",
        "        nn.add(Dense(neurons, input_dim=10, activation=activation))\n",
        "        if normalization > 0.5:\n",
        "            nn.add(BatchNormalization())\n",
        "        for i in range(layers1):\n",
        "            nn.add(Dense(neurons, activation=activation))\n",
        "        if dropout > 0.5:\n",
        "            nn.add(Dropout(dropout_rate, seed=123))\n",
        "        for i in range(layers2):\n",
        "            nn.add(Dense(neurons, activation=activation))\n",
        "        nn.add(Dense(1, activation='sigmoid'))\n",
        "        nn.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return nn\n",
        "    es = EarlyStopping(monitor='accuracy', mode='max', verbose=0, patience=20)\n",
        "    nn = KerasClassifier(build_fn=nn_cl_fun, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
        "    score = cross_val_score(nn, X_train, y_train, scoring=score_acc, cv=kfold, fit_params={'callbacks':[es]}).mean()\n",
        "    return score\n",
        "\n",
        "#The following code searches for the optimum hyperparameters and layers for the Neural Network model.\n",
        "\n",
        "params_nn2 ={\n",
        "    'neurons': (10, 100),\n",
        "    'activation':(0, 9),\n",
        "    'optimizer':(0,7),\n",
        "    'learning_rate':(0.01, 1),\n",
        "    'batch_size':(200, 1000),\n",
        "    'epochs':(20, 100),\n",
        "    'layers1':(1,3),\n",
        "    'layers2':(1,3),\n",
        "    'normalization':(0,1),\n",
        "    'dropout':(0,1),\n",
        "    'dropout_rate':(0,0.3)\n",
        "}\n",
        "# Run Bayesian Optimization\n",
        "nn_bo = BayesianOptimization(nn_cl_bo2, params_nn2, random_state=111)\n",
        "nn_bo.maximize(init_points=25, n_iter=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XDpGNHcDdiJi",
        "outputId": "4886d47a-4a1e-4e3a-8477-8db9ce3a5604",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'activation': 'elu',\n",
              " 'batch_size': 335,\n",
              " 'dropout': 0.4360590193711702,\n",
              " 'dropout_rate': 0.23077874175693686,\n",
              " 'epochs': 44,\n",
              " 'layers1': 1,\n",
              " 'layers2': 1,\n",
              " 'learning_rate': 0.42602224734191213,\n",
              " 'neurons': 31,\n",
              " 'normalization': 0.33765619188879237,\n",
              " 'optimizer': <keras.optimizers.optimizer_v2.ftrl.Ftrl at 0x7f49ac7f2250>}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "#Here are the tuned hyperparameters and layers.\n",
        "params_nn_ = nn_bo.max['params']\n",
        "learning_rate = params_nn_['learning_rate']\n",
        "activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
        "               'elu', 'exponential', LeakyReLU,'relu']\n",
        "params_nn_['activation'] = activationL[round(params_nn_['activation'])]\n",
        "params_nn_['batch_size'] = round(params_nn_['batch_size'])\n",
        "params_nn_['epochs'] = round(params_nn_['epochs'])\n",
        "params_nn_['layers1'] = round(params_nn_['layers1'])\n",
        "params_nn_['layers2'] = round(params_nn_['layers2'])\n",
        "params_nn_['neurons'] = round(params_nn_['neurons'])\n",
        "optimizerL = ['Adam', 'SGD', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl','Adam']\n",
        "optimizerD= {'Adam':Adam(lr=learning_rate), 'SGD':SGD(lr=learning_rate),\n",
        "             'RMSprop':RMSprop(lr=learning_rate), 'Adadelta':Adadelta(lr=learning_rate),\n",
        "             'Adagrad':Adagrad(lr=learning_rate), 'Adamax':Adamax(lr=learning_rate),\n",
        "             'Nadam':Nadam(lr=learning_rate), 'Ftrl':Ftrl(lr=learning_rate)}\n",
        "params_nn_['optimizer'] = optimizerD[optimizerL[round(params_nn_['optimizer'])]]\n",
        "params_nn_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "nL0AkZiji_nV",
        "outputId": "b039781e-ac0d-48ab-954f-7c3734333039",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/44\n",
            "24/24 [==============================] - 1s 13ms/step - loss: -11880.1074 - accuracy: 0.2571 - val_loss: -35509.9961 - val_accuracy: 0.0880\n",
            "Epoch 2/44\n",
            "24/24 [==============================] - 0s 4ms/step - loss: -27834.3965 - accuracy: 0.2716 - val_loss: -55568.2227 - val_accuracy: 0.0880\n",
            "Epoch 3/44\n",
            "24/24 [==============================] - 0s 4ms/step - loss: -39664.2227 - accuracy: 0.2716 - val_loss: -74342.3984 - val_accuracy: 0.0880\n",
            "Epoch 4/44\n",
            "24/24 [==============================] - 0s 4ms/step - loss: -51043.4766 - accuracy: 0.2716 - val_loss: -92566.7031 - val_accuracy: 0.0880\n",
            "Epoch 5/44\n",
            "24/24 [==============================] - 0s 4ms/step - loss: -62181.5391 - accuracy: 0.2716 - val_loss: -110814.5156 - val_accuracy: 0.0880\n",
            "Epoch 6/44\n",
            "24/24 [==============================] - 0s 4ms/step - loss: -73510.0234 - accuracy: 0.2716 - val_loss: -129565.8750 - val_accuracy: 0.0880\n",
            "Epoch 7/44\n",
            "24/24 [==============================] - 0s 4ms/step - loss: -85224.9922 - accuracy: 0.2716 - val_loss: -149008.7344 - val_accuracy: 0.0880\n",
            "Epoch 8/44\n",
            "24/24 [==============================] - 0s 4ms/step - loss: -97379.5078 - accuracy: 0.2716 - val_loss: -169205.1875 - val_accuracy: 0.0880\n",
            "Epoch 9/44\n",
            "24/24 [==============================] - 0s 4ms/step - loss: -110004.2734 - accuracy: 0.2716 - val_loss: -190151.9531 - val_accuracy: 0.0880\n",
            "Epoch 10/44\n",
            "24/24 [==============================] - 0s 4ms/step - loss: -123080.5625 - accuracy: 0.2716 - val_loss: -211826.4688 - val_accuracy: 0.0880\n",
            "Epoch 11/44\n",
            "24/24 [==============================] - 0s 4ms/step - loss: -136618.8125 - accuracy: 0.2716 - val_loss: -234268.8750 - val_accuracy: 0.0880\n",
            "Epoch 12/44\n",
            "24/24 [==============================] - 0s 4ms/step - loss: -150624.7656 - accuracy: 0.2716 - val_loss: -257472.4844 - val_accuracy: 0.0880\n",
            "Epoch 13/44\n",
            "24/24 [==============================] - 0s 4ms/step - loss: -165092.3750 - accuracy: 0.2716 - val_loss: -281399.0625 - val_accuracy: 0.0880\n",
            "Epoch 14/44\n",
            "24/24 [==============================] - 0s 4ms/step - loss: -180011.2031 - accuracy: 0.2716 - val_loss: -306077.6562 - val_accuracy: 0.0880\n",
            "Epoch 15/44\n",
            "24/24 [==============================] - 0s 4ms/step - loss: -195385.5938 - accuracy: 0.2716 - val_loss: -331489.0000 - val_accuracy: 0.0880\n",
            "Epoch 16/44\n",
            "24/24 [==============================] - 0s 4ms/step - loss: -211207.0625 - accuracy: 0.2716 - val_loss: -357610.1250 - val_accuracy: 0.0880\n",
            "Epoch 17/44\n",
            "24/24 [==============================] - 0s 4ms/step - loss: -227472.6875 - accuracy: 0.2716 - val_loss: -384467.1562 - val_accuracy: 0.0880\n",
            "Epoch 18/44\n",
            "24/24 [==============================] - 0s 4ms/step - loss: -244182.7188 - accuracy: 0.2716 - val_loss: -412046.4688 - val_accuracy: 0.0880\n",
            "Epoch 19/44\n",
            "24/24 [==============================] - 0s 4ms/step - loss: -261229.3125 - accuracy: 0.2716 - val_loss: -439901.4062 - val_accuracy: 0.0880\n",
            "Epoch 20/44\n",
            "24/24 [==============================] - 0s 4ms/step - loss: -278471.1250 - accuracy: 0.2716 - val_loss: -468268.2812 - val_accuracy: 0.0880\n",
            "Epoch 21/44\n",
            "24/24 [==============================] - 0s 3ms/step - loss: -296071.9688 - accuracy: 0.2716 - val_loss: -497193.0312 - val_accuracy: 0.0880\n",
            "Epoch 22/44\n",
            "24/24 [==============================] - 0s 4ms/step - loss: -314025.2188 - accuracy: 0.2716 - val_loss: -526748.5000 - val_accuracy: 0.0880\n",
            "Epoch 23/44\n",
            "24/24 [==============================] - 0s 4ms/step - loss: -332356.0312 - accuracy: 0.2716 - val_loss: -556872.8750 - val_accuracy: 0.0880\n",
            "Epoch 24/44\n",
            "24/24 [==============================] - 0s 4ms/step - loss: -350877.4375 - accuracy: 0.2716 - val_loss: -587002.5625 - val_accuracy: 0.0880\n",
            "Epoch 25/44\n",
            "24/24 [==============================] - 0s 4ms/step - loss: -369513.8750 - accuracy: 0.2716 - val_loss: -617576.0625 - val_accuracy: 0.0880\n",
            "Epoch 26/44\n",
            "24/24 [==============================] - 0s 4ms/step - loss: -388448.8438 - accuracy: 0.2716 - val_loss: -648641.0625 - val_accuracy: 0.0880\n",
            "Epoch 27/44\n",
            "24/24 [==============================] - 0s 4ms/step - loss: -407696.9688 - accuracy: 0.2716 - val_loss: -680236.5625 - val_accuracy: 0.0880\n",
            "Epoch 28/44\n",
            "24/24 [==============================] - 0s 4ms/step - loss: -427267.1250 - accuracy: 0.2716 - val_loss: -712381.8125 - val_accuracy: 0.0880\n",
            "Epoch 29/44\n",
            "24/24 [==============================] - 0s 4ms/step - loss: -447179.4688 - accuracy: 0.2716 - val_loss: -745030.9375 - val_accuracy: 0.0880\n",
            "Epoch 30/44\n",
            "24/24 [==============================] - 0s 4ms/step - loss: -467404.1875 - accuracy: 0.2716 - val_loss: -778237.1875 - val_accuracy: 0.0880\n",
            "Epoch 31/44\n",
            "24/24 [==============================] - 0s 4ms/step - loss: -487970.1250 - accuracy: 0.2716 - val_loss: -811943.6875 - val_accuracy: 0.0880\n",
            "Epoch 32/44\n",
            "24/24 [==============================] - 0s 4ms/step - loss: -508844.8125 - accuracy: 0.2716 - val_loss: -846191.4375 - val_accuracy: 0.0880\n",
            "Epoch 33/44\n",
            "24/24 [==============================] - 0s 4ms/step - loss: -530054.9375 - accuracy: 0.2716 - val_loss: -881011.6875 - val_accuracy: 0.0880\n",
            "Epoch 34/44\n",
            "24/24 [==============================] - 0s 4ms/step - loss: -551607.4375 - accuracy: 0.2716 - val_loss: -916316.0625 - val_accuracy: 0.0880\n",
            "Epoch 35/44\n",
            "24/24 [==============================] - 0s 4ms/step - loss: -573466.1875 - accuracy: 0.2716 - val_loss: -952173.6875 - val_accuracy: 0.0880\n",
            "Epoch 36/44\n",
            "24/24 [==============================] - 0s 4ms/step - loss: -595658.2500 - accuracy: 0.2716 - val_loss: -988517.1250 - val_accuracy: 0.0880\n",
            "Epoch 37/44\n",
            "24/24 [==============================] - 0s 4ms/step - loss: -618156.3125 - accuracy: 0.2716 - val_loss: -1025396.6250 - val_accuracy: 0.0880\n",
            "Epoch 38/44\n",
            "24/24 [==============================] - 0s 4ms/step - loss: -640984.2500 - accuracy: 0.2716 - val_loss: -1062840.1250 - val_accuracy: 0.0880\n",
            "Epoch 39/44\n",
            "24/24 [==============================] - 0s 4ms/step - loss: -664156.2500 - accuracy: 0.2716 - val_loss: -1100762.0000 - val_accuracy: 0.0880\n",
            "Epoch 40/44\n",
            "24/24 [==============================] - 0s 4ms/step - loss: -687617.6875 - accuracy: 0.2716 - val_loss: -1139208.7500 - val_accuracy: 0.0880\n",
            "Epoch 41/44\n",
            "24/24 [==============================] - 0s 4ms/step - loss: -711409.6875 - accuracy: 0.2716 - val_loss: -1178174.7500 - val_accuracy: 0.0880\n",
            "Epoch 42/44\n",
            "24/24 [==============================] - 0s 4ms/step - loss: -735518.3125 - accuracy: 0.2716 - val_loss: -1217662.0000 - val_accuracy: 0.0880\n",
            "Epoch 43/44\n",
            "24/24 [==============================] - 0s 4ms/step - loss: -759944.2500 - accuracy: 0.2716 - val_loss: -1257691.6250 - val_accuracy: 0.0880\n",
            "Epoch 44/44\n",
            "24/24 [==============================] - 0s 4ms/step - loss: -784708.2500 - accuracy: 0.2716 - val_loss: -1298237.0000 - val_accuracy: 0.0880\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f49ac420e90>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "#Fitting Neural Network\n",
        "def nn_cl_fun():\n",
        "  nn = Sequential()\n",
        "  nn.add(Dense(params_nn_['neurons'], input_dim=25, activation=params_nn_['activation']))\n",
        "  if params_nn_['normalization'] > 0.5:\n",
        "    nn.add(BatchNormalization())\n",
        "  for i in range(params_nn_['layers1']):\n",
        "    nn.add(Dense(params_nn_['neurons'], activation=params_nn_['activation']))\n",
        "  if params_nn_['dropout'] > 0.5:\n",
        "    nn.add(Dropout(params_nn_['dropout_rate'], seed=123))\n",
        "  for i in range(params_nn_['layers2']):\n",
        "    nn.add(Dense(params_nn_['neurons'], activation=params_nn_['activation']))\n",
        "  nn.add(Dense(1, activation='sigmoid'))\n",
        "  nn.compile(loss='binary_crossentropy', optimizer=params_nn_['optimizer'], metrics=['accuracy'])\n",
        "  return nn\n",
        "es = EarlyStopping(monitor='accuracy', mode='max', verbose=0, patience=20)\n",
        "nn = KerasClassifier(build_fn=nn_cl_fun, epochs=params_nn_['epochs'], batch_size=params_nn_['batch_size'],\n",
        "                         verbose=0)\n",
        "nn.fit(X_train, y_train, validation_data=(X_test, y_test), verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WNLrAxRlGSJ"
      },
      "outputs": [],
      "source": [
        "print('X_train dimension= ', X_train.shape)\n",
        "print('X_test dimension= ', X_test.shape)\n",
        "print('y_train dimension= ', y_train.shape)\n",
        "print('y_test dimension= ', y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXBTNcNHl8SB"
      },
      "outputs": [],
      "source": [
        "# import tensorflow and fix the random seed for better reproducibility\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(42)\n",
        "# import the necessary packages\n",
        "\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from tensorflow.keras.datasets import mnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfaiylXjsWli"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "def get_mlp_model(hiddenLayerOne=784, hiddenLayerTwo=256,\n",
        "\tdropout=0.2, learnRate=0.01):\n",
        "\t# initialize a sequential model and add layer to flatten the\n",
        "\t# input data\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Flatten())\n",
        "  \t# add two stacks of FC => RELU => DROPOUT\n",
        "\tmodel.add(Dense(hiddenLayerOne, activation=\"relu\",\n",
        "\t\tinput_shape=(784,)))\n",
        "\tmodel.add(Dropout(dropout))\n",
        "\tmodel.add(Dense(hiddenLayerTwo, activation=\"relu\"))\n",
        "\tmodel.add(Dropout(dropout))\n",
        "\t# add a softmax layer on top\n",
        "\tmodel.add(Dense(10, activation=\"softmax\"))\n",
        "\t# compile the model\n",
        "\tmodel.compile(\n",
        "\t\toptimizer=Adam(learning_rate=learnRate),\n",
        "\t\tloss=\"sparse_categorical_crossentropy\",\n",
        "\t\tmetrics=[\"accuracy\"])\n",
        "\t# return compiled model\n",
        "\treturn model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8fqOeoVsp4u"
      },
      "outputs": [],
      "source": [
        "# initialize our model with the default hyperparameter values\n",
        "print(\"[INFO] initializing model...\")\n",
        "model = get_mlp_model()\n",
        "# train the network (i.e., no hyperparameter tuning)\n",
        "print(\"[INFO] training model...\")\n",
        "H = model.fit(X_train,y_train,\n",
        "\tvalidation_data=(X_test,y_test),\n",
        "\tbatch_size=8,\n",
        "\tepochs=20)\n",
        "# make predictions on the test set and evaluate it\n",
        "print(\"[INFO] evaluating network...\")\n",
        "accuracy = model.evaluate(X_test,y_test)[1]\n",
        "print(\"accuracy: {:.2f}%\".format(accuracy * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuuCi_AAn1I2"
      },
      "outputs": [],
      "source": [
        "print(\"[INFO] initializing model...\")\n",
        "model = KerasClassifier(build_fn=get_mlp_model, verbose=0)\n",
        "# define a grid of the hyperparameter search space\n",
        "hiddenLayerOne = [256, 512, 784]\n",
        "hiddenLayerTwo = [128, 256, 512]\n",
        "learnRate = [1e-2, 1e-3, 1e-4]\n",
        "dropout = [0.3, 0.4, 0.5]\n",
        "batchSize = [4, 8, 16, 32]\n",
        "epochs = [10, 20, 30, 40]\n",
        "# create a dictionary from the hyperparameter grid\n",
        "grid = dict(\n",
        "\thiddenLayerOne=hiddenLayerOne,\n",
        "\tlearnRate=learnRate,\n",
        "\thiddenLayerTwo=hiddenLayerTwo,\n",
        "\tdropout=dropout,\n",
        "\tbatch_size=batchSize,\n",
        "\tepochs=epochs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcXXiTrjoa6Q"
      },
      "outputs": [],
      "source": [
        "# initialize a random search with a 3-fold cross-validation and then\n",
        "# start the hyperparameter search process\n",
        "print(\"[INFO] performing random search...\")\n",
        "searcher = RandomizedSearchCV(estimator=model, n_jobs=-1, cv=3,\n",
        "\tparam_distributions=grid, scoring=\"accuracy\")\n",
        "searchResults = searcher.fit(X_train,y_train)\n",
        "# summarize grid search information\n",
        "bestScore = searchResults.best_score_\n",
        "bestParams = searchResults.best_params_\n",
        "print(\"[INFO] best score is {:.2f} using {}\".format(bestScore,\n",
        "\tbestParams))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZ5QdlbaoeTu"
      },
      "outputs": [],
      "source": [
        "# extract the best model, make predictions on our data, and show a\n",
        "# classification report\n",
        "print(\"[INFO] evaluating the best model...\")\n",
        "bestModel = searchResults.best_estimator_\n",
        "accuracy = bestModel.score(X_train,y_train)\n",
        "print(\"accuracy: {:.2f}%\".format(accuracy * 100))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GVGklrWnkiu1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}